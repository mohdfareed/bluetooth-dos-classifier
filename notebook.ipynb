{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bluetooth DoS Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz, save_npz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = \"experiments\"\n",
    "if not os.path.exists(EXPERIMENTS):\n",
    "    os.makedirs(\"experiments\")\n",
    "\n",
    "# get number of experiments (max + 1 is the next experiment number)\n",
    "experiment_files = [f for f in os.listdir(EXPERIMENTS) if f.endswith(\".log\")]\n",
    "experiment = (\n",
    "    max([int(f.split(\".\")[0]) for f in experiment_files])\n",
    "    if experiment_files\n",
    "    else 0\n",
    ") + 1\n",
    "\n",
    "# create an experiments logger\n",
    "logger = logging.getLogger(\"experiment_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# create file handler\n",
    "file_handler = logging.FileHandler(f\"experiments/{experiment}.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_formatter = logging.Formatter(\"[%(asctime)s] %(message)s\")\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# create console handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_formatter = logging.Formatter(\"%(message)s\")\n",
    "stream_handler.setFormatter(stream_formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "@contextmanager  # redirect stdout to the logger\n",
    "def log_redirector(logger):\n",
    "    class StreamToLogger(object):\n",
    "        \"\"\"Fake file-like stream object that redirects writes to a logger instance.\"\"\"\n",
    "\n",
    "        def __init__(self, logger, level):\n",
    "            self.logger = logger\n",
    "            self.level = level\n",
    "\n",
    "        def write(self, buffer):\n",
    "            for line in buffer.rstrip().splitlines():\n",
    "                self.logger.log(self.level, line.rstrip())\n",
    "\n",
    "        def flush(self):\n",
    "            pass\n",
    "\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StreamToLogger(logger, logging.INFO)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data\"\n",
    "MODELS = \"models\"\n",
    "\n",
    "# data paths\n",
    "ATTACK_TRAIN = os.path.join(DATA, \"dos_train.csv\")\n",
    "BENIGN_TRAIN = os.path.join(DATA, \"benign_train.csv\")\n",
    "ATTACK_TEST = os.path.join(DATA, \"dos_test.csv\")\n",
    "BENIGN_TEST = os.path.join(DATA, \"benign_test.csv\")\n",
    "\n",
    "# preprocessed data paths\n",
    "PREPROCESSED_TRAIN = os.path.join(DATA, \"preprocessed_train.csv\")\n",
    "PREPROCESSED_TEST = os.path.join(DATA, \"preprocessed_test.csv\")\n",
    "LABELS_TRAIN = os.path.join(DATA, \"labels_train.npy\")\n",
    "LABELS_TEST = os.path.join(DATA, \"labels_test.npy\")\n",
    "\n",
    "# features paths\n",
    "FEATURES_TEST = os.path.join(DATA, f\"features_test_{experiment}.npz\")\n",
    "FEATURES_TRAIN = os.path.join(DATA, f\"features_train_{experiment}.npz\")\n",
    "\n",
    "# models paths\n",
    "VECTORIZER_MODEL = os.path.join(MODELS, f\"vectorizer_{experiment}.joblib\")\n",
    "ENCODER_MODEL = os.path.join(MODELS, f\"encoder_{experiment}.joblib\")\n",
    "SCALER_MODEL = os.path.join(MODELS, f\"scaler_{experiment}.joblib\")\n",
    "GBM_MODEL = os.path.join(\n",
    "    MODELS, f\"gbm_{experiment}.joblib\"\n",
    ")  # gradient boosting machine\n",
    "\n",
    "# ensure that the working directory is the same as the notebook\n",
    "notebook_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "os.chdir(notebook_path)\n",
    "\n",
    "# create directories\n",
    "if not os.path.exists(DATA):\n",
    "    raise Exception(\"Data directory not found.\")\n",
    "if not os.path.exists(MODELS):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is obtained from the following link:\n",
    "https://www.unb.ca/cic/datasets/iomt-dataset-2024.html\n",
    "\n",
    "The specific dataset used is the \"Bluetooth\" dataset. The dataset is in `.pcap` format. The dataset is first converted to `.csv` format using the `tshark` command line tool (WireShark can also be used). The resulting dataset files are:\n",
    "\n",
    "- `data/benign_test.csv`\n",
    "- `data/benign_train.csv`\n",
    "- `data/dos_test.csv`\n",
    "- `data/dos_train.csv`\n",
    "\n",
    "The original dataset are compressed in their `.pcap` format at `data/bl_dataset.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "attack_train = pd.read_csv(ATTACK_TRAIN)\n",
    "benign_train = pd.read_csv(BENIGN_TRAIN)\n",
    "attack_test = pd.read_csv(ATTACK_TEST)\n",
    "benign_test = pd.read_csv(BENIGN_TEST)\n",
    "\n",
    "# add type column indicating attack or benign\n",
    "attack_train[\"Type\"] = 1\n",
    "attack_test[\"Type\"] = 1\n",
    "benign_train[\"Type\"] = 0\n",
    "benign_test[\"Type\"] = 0\n",
    "\n",
    "# combine datasets\n",
    "train_dataset = pd.concat([attack_train, benign_train], ignore_index=True)\n",
    "test_dataset = pd.concat([attack_test, benign_test], ignore_index=True)\n",
    "\n",
    "# shuffle datasets\n",
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# generate labels\n",
    "train_labels = train_dataset[\"Type\"]\n",
    "train_dataset.drop(columns=[\"Type\"], inplace=True)\n",
    "test_labels = test_dataset[\"Type\"]\n",
    "test_dataset.drop(columns=[\"Type\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "                 No.          Time         Length      Type\n",
      "count  998391.000000  9.983910e+05  998391.000000  998391.0\n",
      "mean   499196.000000  4.621494e+05      20.064474       1.0\n",
      "std    288210.800641  4.679402e+05      12.352341       0.0\n",
      "min         1.000000  0.000000e+00       4.000000       1.0\n",
      "25%    249598.500000  2.276077e+03       8.000000       1.0\n",
      "50%    499196.000000  6.853190e+05      16.000000       1.0\n",
      "75%    748793.500000  8.540422e+05      32.000000       1.0\n",
      "max    998391.000000  1.198804e+06     255.000000       1.0\n",
      "\n",
      "Testing data:\n",
      "                 No.           Time         Length      Type\n",
      "count  251708.000000  251708.000000  251708.000000  251708.0\n",
      "mean   125854.500000    1071.256077      20.006591       1.0\n",
      "std     72661.985116     638.578923      12.006976       0.0\n",
      "min         1.000000       0.000000       5.000000       1.0\n",
      "25%     62927.750000     601.485238       8.000000       1.0\n",
      "50%    125854.500000    1050.763037      16.000000       1.0\n",
      "75%    188781.250000    1669.751179      32.000000       1.0\n",
      "max    251708.000000    2094.097550      46.000000       1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics\n",
    "logger.info(f\"Training data:\\n{attack_train.describe()}\\n\")\n",
    "logger.info(f\"Testing data:\\n{attack_test.describe()}\\n\")\n",
    "\n",
    "# write modified dataset to files\n",
    "train_dataset.to_csv(PREPROCESSED_TRAIN, index=False)\n",
    "test_dataset.to_csv(PREPROCESSED_TEST, index=False)\n",
    "np.save(LABELS_TRAIN, train_labels)\n",
    "np.save(LABELS_TEST, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_hashing(dataset, column, n_features=20):\n",
    "    \"\"\"Applies feature hashing to a specified column of the dataset.\"\"\"\n",
    "    hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
    "    hashed_features = hasher.transform(\n",
    "        dataset[column].apply(lambda x: [str(x)])\n",
    "    )\n",
    "    return hashed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TF-IDF vectorization...\n",
      "Applying one-hot encoding...\n",
      "Applying standard scaling...\n",
      "Applying feature hashing...\n"
     ]
    }
   ],
   "source": [
    "# apply tf-idf vectorization to Info column\n",
    "print(\"Applying TF-IDF vectorization...\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_info = vectorizer.fit_transform(train_dataset[\"Info\"])\n",
    "test_info = vectorizer.transform(test_dataset[\"Info\"])\n",
    "\n",
    "# apply one-hot encoding to Protocol column\n",
    "print(\"Applying one-hot encoding...\")\n",
    "encoder = OneHotEncoder()\n",
    "train_protocol = encoder.fit_transform(train_dataset[[\"Protocol\"]])\n",
    "test_protocol = encoder.transform(test_dataset[[\"Protocol\"]])\n",
    "\n",
    "# apply standard scaling to Length column\n",
    "print(\"Applying standard scaling...\")\n",
    "scaler = StandardScaler()\n",
    "train_length = scaler.fit_transform(train_dataset[[\"Length\"]])\n",
    "test_length = scaler.transform(test_dataset[[\"Length\"]])\n",
    "\n",
    "# apply feature hashing to Source and Destination columns\n",
    "print(\"Applying feature hashing...\")\n",
    "train_source = apply_feature_hashing(train_dataset, \"Source\")\n",
    "test_source = apply_feature_hashing(test_dataset, \"Source\")\n",
    "train_destination = apply_feature_hashing(train_dataset, \"Destination\")\n",
    "test_destination = apply_feature_hashing(test_dataset, \"Destination\")\n",
    "\n",
    "# combine features\n",
    "train_features = hstack(\n",
    "    [\n",
    "        csr_matrix(train_dataset[[\"Time\"]]),\n",
    "        train_source,\n",
    "        train_destination,\n",
    "        train_protocol,\n",
    "        csr_matrix(train_length),\n",
    "        train_info,\n",
    "    ]\n",
    ")\n",
    "test_features = hstack(\n",
    "    [\n",
    "        csr_matrix(test_dataset[[\"Time\"]]),\n",
    "        test_source,\n",
    "        test_destination,\n",
    "        test_protocol,\n",
    "        csr_matrix(test_length),\n",
    "        test_info,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary size: 609\n",
      "One-Hot Encoding unique categories: 6\n",
      "Standard Scaling mean: 22.7672\n",
      "Standard Scaling std: 14.1469\n",
      "Feature Hashing features count: 20\n",
      "Total number of features: 657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# report feature extraction results\n",
    "logger.info(f\"TF-IDF Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "logger.info(f\"One-Hot Encoding unique categories: {len(encoder.categories_[0])}\")  # type: ignore\n",
    "logger.info(f\"Standard Scaling mean: {scaler.mean_[0]:.4f}\")  # type: ignore\n",
    "logger.info(f\"Standard Scaling std: {scaler.scale_[0]:.4f}\")  # type: ignore\n",
    "logger.info(f\"Feature Hashing features count: {train_source.shape[1]}\")\n",
    "logger.info(f\"Total number of features: {train_features.shape[1]}\\n\")\n",
    "\n",
    "# write features and models to files\n",
    "save_npz(FEATURES_TRAIN, train_features)\n",
    "save_npz(FEATURES_TEST, test_features)\n",
    "_ = dump(vectorizer, VECTORIZER_MODEL)\n",
    "_ = dump(encoder, ENCODER_MODEL)\n",
    "_ = dump(scaler, SCALER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time\n",
      "         1           0.7654            3.36m\n",
      "         2           0.6565            3.13m\n",
      "         3           0.5758            3.07m\n",
      "         4           0.5121            3.06m\n",
      "         5           0.4581            2.94m\n",
      "         6           0.4140            2.86m\n",
      "         7           0.3766            2.82m\n",
      "         8           0.3440            2.80m\n",
      "         9           0.3160            2.74m\n",
      "        10           0.2917            2.79m\n",
      "        20           0.1581            2.46m\n",
      "        30           0.0744            2.12m\n",
      "        40           0.0528            1.82m\n",
      "        50           0.0317            1.54m\n",
      "        60           0.0204            1.23m\n",
      "        70           0.0173           55.84s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_redirector(logger):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained complete.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# write model to file\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/school/bluetooth-dos-classifier/.venv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/school/bluetooth-dos-classifier/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:784\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    783\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 784\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Developer/school/bluetooth-dos-classifier/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:880\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    873\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[1;32m    874\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[1;32m    875\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    876\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[1;32m    877\u001b[0m         )\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/Developer/school/bluetooth-dos-classifier/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:454\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    444\u001b[0m     set_huber_delta(\n\u001b[1;32m    445\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss,\n\u001b[1;32m    446\u001b[0m         y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    447\u001b[0m         raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions,\n\u001b[1;32m    448\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# TODO: Without oob, i.e. with self.subsample = 1.0, we could call\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# self._loss.loss_gradient and use it to set train_score_.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# But note that train_score_[i] is the score AFTER fitting the i-th tree.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Note: We need the negative gradient!\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m neg_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We pass sample_weights to the tree directly.\u001b[39;49;00m\n\u001b[1;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# on neg_gradient to simplify the loop over n_trees_per_iteration_.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neg_gradient\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/school/bluetooth-dos-classifier/.venv/lib/python3.12/site-packages/sklearn/_loss/loss.py:304\u001b[0m, in \u001b[0;36mBaseLoss.gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    302\u001b[0m     gradient_out \u001b[38;5;241m=\u001b[39m gradient_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient_out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load features (to prevent forced extraction to define variables)\n",
    "train_features = load_npz(FEATURES_TRAIN)\n",
    "train_labels = np.load(LABELS_TRAIN)\n",
    "\n",
    "# train model\n",
    "model = GradientBoostingClassifier(verbose=1)\n",
    "with log_redirector(logger):\n",
    "    model.fit(train_features, train_labels)  # type: ignore\n",
    "logger.info(\"Model trained complete.\\n\")\n",
    "\n",
    "# write model to file\n",
    "_ = dump(model, GBM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8133819920640428\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 56898   8432]\n",
      " [ 50733 200975]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.87      0.66     65330\n",
      "           1       0.96      0.80      0.87    251708\n",
      "\n",
      "    accuracy                           0.81    317038\n",
      "   macro avg       0.74      0.83      0.76    317038\n",
      "weighted avg       0.87      0.81      0.83    317038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and features (to prevent forced training to define variables)\n",
    "model = load(GBM_MODEL)\n",
    "test_features = load_npz(FEATURES_TEST)\n",
    "test_labels = np.load(LABELS_TEST)\n",
    "\n",
    "# evaluate model\n",
    "predictions = model.predict(test_features)  # type: ignore\n",
    "accuracy = metrics.accuracy_score(test_labels, predictions)\n",
    "logger.info(f\"Train accuracy: {accuracy}\")\n",
    "conf_matrix = metrics.confusion_matrix(test_labels, predictions)\n",
    "logger.info(f\"Confusion matrix:\\n{conf_matrix}\")\n",
    "report = metrics.classification_report(test_labels, predictions)\n",
    "logger.info(f\"Classification report:\\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
