{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bluetooth DoS Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz, save_npz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting experiment 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENTS = \"experiments\"\n",
    "if not os.path.exists(EXPERIMENTS):\n",
    "    os.makedirs(\"experiments\")\n",
    "\n",
    "# get number of experiments (max + 1 is the next experiment number)\n",
    "experiment_files = [f for f in os.listdir(EXPERIMENTS) if f.endswith(\".log\")]\n",
    "experiment = (\n",
    "    max([int(f.split(\".\")[0]) for f in experiment_files])\n",
    "    if experiment_files\n",
    "    else 0\n",
    ") + 1\n",
    "\n",
    "# create an experiments logger\n",
    "logger = logging.getLogger(\"experiment_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# create file handler\n",
    "file_handler = logging.FileHandler(f\"experiments/{experiment}.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_formatter = logging.Formatter(\"[%(asctime)s] %(message)s\")\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# create console handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_formatter = logging.Formatter(\"%(message)s\")\n",
    "stream_handler.setFormatter(stream_formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.info(f\"Starting experiment {experiment}\\n\")\n",
    "\n",
    "\n",
    "@contextmanager  # redirect stdout to the logger\n",
    "def log_redirector(logger):\n",
    "    class StreamToLogger(object):\n",
    "        \"\"\"Fake file-like stream object that redirects writes to a logger instance.\"\"\"\n",
    "\n",
    "        def __init__(self, logger, level):\n",
    "            self.logger = logger\n",
    "            self.level = level\n",
    "\n",
    "        def write(self, buffer):\n",
    "            for line in buffer.rstrip().splitlines():\n",
    "                self.logger.log(self.level, line.rstrip())\n",
    "\n",
    "        def flush(self):\n",
    "            pass\n",
    "\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StreamToLogger(logger, logging.INFO)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data\"\n",
    "MODELS = \"models\"\n",
    "\n",
    "# data paths\n",
    "ATTACK_TRAIN = os.path.join(DATA, \"dos_train.csv\")\n",
    "BENIGN_TRAIN = os.path.join(DATA, \"benign_train.csv\")\n",
    "ATTACK_TEST = os.path.join(DATA, \"dos_test.csv\")\n",
    "BENIGN_TEST = os.path.join(DATA, \"benign_test.csv\")\n",
    "\n",
    "# preprocessed data paths\n",
    "PREPROCESSED_TRAIN = os.path.join(DATA, \"preprocessed_train.csv\")\n",
    "PREPROCESSED_TEST = os.path.join(DATA, \"preprocessed_test.csv\")\n",
    "LABELS_TRAIN = os.path.join(DATA, f\"labels_train_{experiment}.npy\")\n",
    "LABELS_TEST = os.path.join(DATA, f\"labels_test_{experiment}.npy\")\n",
    "\n",
    "# features paths\n",
    "FEATURES_TEST = os.path.join(DATA, f\"features_test_{experiment}.npz\")\n",
    "FEATURES_TRAIN = os.path.join(DATA, f\"features_train_{experiment}.npz\")\n",
    "\n",
    "# models paths\n",
    "VECTORIZER_MODEL = os.path.join(MODELS, f\"vectorizer_{experiment}.joblib\")\n",
    "ENCODER_MODEL = os.path.join(MODELS, f\"encoder_{experiment}.joblib\")\n",
    "SCALER_MODEL = os.path.join(MODELS, f\"scaler_{experiment}.joblib\")\n",
    "GBM_MODEL = os.path.join(\n",
    "    MODELS, f\"gbm_{experiment}.joblib\"\n",
    ")  # gradient boosting machine\n",
    "\n",
    "# ensure that the working directory is the same as the notebook\n",
    "notebook_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "os.chdir(notebook_path)\n",
    "\n",
    "# create directories\n",
    "if not os.path.exists(DATA):\n",
    "    raise Exception(\"Data directory not found.\")\n",
    "if not os.path.exists(MODELS):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is obtained from the following link:\n",
    "https://www.unb.ca/cic/datasets/iomt-dataset-2024.html\n",
    "\n",
    "The specific dataset used is the \"Bluetooth\" dataset. The dataset is in `.pcap` format. The dataset is first converted to `.csv` format using the `tshark` command line tool (WireShark can also be used). The resulting dataset files are:\n",
    "\n",
    "- `data/benign_test.csv`\n",
    "- `data/benign_train.csv`\n",
    "- `data/dos_test.csv`\n",
    "- `data/dos_train.csv`\n",
    "\n",
    "The original dataset are compressed in their `.pcap` format at `data/bl_dataset.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "attack_train = pd.read_csv(ATTACK_TRAIN)\n",
    "benign_train = pd.read_csv(BENIGN_TRAIN)\n",
    "attack_test = pd.read_csv(ATTACK_TEST)\n",
    "benign_test = pd.read_csv(BENIGN_TEST)\n",
    "\n",
    "# add type column indicating attack or benign\n",
    "attack_train[\"Type\"] = 1\n",
    "attack_test[\"Type\"] = 1\n",
    "benign_train[\"Type\"] = 0\n",
    "benign_test[\"Type\"] = 0\n",
    "\n",
    "# combine datasets\n",
    "train_dataset = pd.concat([attack_train, benign_train], ignore_index=True)\n",
    "test_dataset = pd.concat([attack_test, benign_test], ignore_index=True)\n",
    "\n",
    "# shuffle datasets\n",
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# generate labels\n",
    "train_labels = train_dataset[\"Type\"]\n",
    "train_dataset.drop(columns=[\"Type\"], inplace=True)\n",
    "test_labels = test_dataset[\"Type\"]\n",
    "test_dataset.drop(columns=[\"Type\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "                 No.          Time         Length      Type\n",
      "count  998391.000000  9.983910e+05  998391.000000  998391.0\n",
      "mean   499196.000000  4.621494e+05      20.064474       1.0\n",
      "std    288210.800641  4.679402e+05      12.352341       0.0\n",
      "min         1.000000  0.000000e+00       4.000000       1.0\n",
      "25%    249598.500000  2.276077e+03       8.000000       1.0\n",
      "50%    499196.000000  6.853190e+05      16.000000       1.0\n",
      "75%    748793.500000  8.540422e+05      32.000000       1.0\n",
      "max    998391.000000  1.198804e+06     255.000000       1.0\n",
      "\n",
      "Testing data:\n",
      "                 No.           Time         Length      Type\n",
      "count  251708.000000  251708.000000  251708.000000  251708.0\n",
      "mean   125854.500000    1071.256077      20.006591       1.0\n",
      "std     72661.985116     638.578923      12.006976       0.0\n",
      "min         1.000000       0.000000       5.000000       1.0\n",
      "25%     62927.750000     601.485238       8.000000       1.0\n",
      "50%    125854.500000    1050.763037      16.000000       1.0\n",
      "75%    188781.250000    1669.751179      32.000000       1.0\n",
      "max    251708.000000    2094.097550      46.000000       1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics\n",
    "logger.info(f\"Training data:\\n{attack_train.describe()}\\n\")\n",
    "logger.info(f\"Testing data:\\n{attack_test.describe()}\\n\")\n",
    "\n",
    "# write modified dataset to files\n",
    "train_dataset.to_csv(PREPROCESSED_TRAIN, index=False)\n",
    "test_dataset.to_csv(PREPROCESSED_TEST, index=False)\n",
    "np.save(LABELS_TRAIN, train_labels)\n",
    "np.save(LABELS_TEST, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_hashing(dataset, column, n_features=20):\n",
    "    \"\"\"Applies feature hashing to a specified column of the dataset.\"\"\"\n",
    "    hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
    "    hashed_features = hasher.transform(\n",
    "        dataset[column].apply(lambda x: [str(x)])\n",
    "    )\n",
    "    return hashed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TF-IDF vectorization...\n",
      "Applying one-hot encoding...\n",
      "Applying standard scaling...\n",
      "Applying feature hashing...\n"
     ]
    }
   ],
   "source": [
    "# apply tf-idf vectorization to Info column\n",
    "print(\"Applying TF-IDF vectorization...\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_info = vectorizer.fit_transform(train_dataset[\"Info\"])\n",
    "test_info = vectorizer.transform(test_dataset[\"Info\"])\n",
    "\n",
    "# apply one-hot encoding to Protocol column\n",
    "print(\"Applying one-hot encoding...\")\n",
    "encoder = OneHotEncoder()\n",
    "train_protocol = encoder.fit_transform(train_dataset[[\"Protocol\"]])\n",
    "test_protocol = encoder.transform(test_dataset[[\"Protocol\"]])\n",
    "\n",
    "# apply standard scaling to Length column\n",
    "print(\"Applying standard scaling...\")\n",
    "scaler = StandardScaler()\n",
    "train_length = scaler.fit_transform(train_dataset[[\"Length\"]])\n",
    "test_length = scaler.transform(test_dataset[[\"Length\"]])\n",
    "\n",
    "# apply feature hashing to Source and Destination columns\n",
    "print(\"Applying feature hashing...\")\n",
    "train_source = apply_feature_hashing(train_dataset, \"Source\")\n",
    "test_source = apply_feature_hashing(test_dataset, \"Source\")\n",
    "train_destination = apply_feature_hashing(train_dataset, \"Destination\")\n",
    "test_destination = apply_feature_hashing(test_dataset, \"Destination\")\n",
    "\n",
    "# combine features\n",
    "train_features = hstack(\n",
    "    [\n",
    "        csr_matrix(train_dataset[[\"Time\"]]),\n",
    "        train_source,\n",
    "        train_destination,\n",
    "        train_protocol,\n",
    "        csr_matrix(train_length),\n",
    "        train_info,\n",
    "    ]\n",
    ")\n",
    "test_features = hstack(\n",
    "    [\n",
    "        csr_matrix(test_dataset[[\"Time\"]]),\n",
    "        test_source,\n",
    "        test_destination,\n",
    "        test_protocol,\n",
    "        csr_matrix(test_length),\n",
    "        test_info,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary size: 609\n",
      "One-Hot Encoding unique categories: 6\n",
      "Standard Scaling mean: 22.7672\n",
      "Standard Scaling std: 14.1469\n",
      "Feature Hashing features count: 20\n",
      "Total number of features: 657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# report feature extraction results\n",
    "logger.info(f\"TF-IDF Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "logger.info(f\"One-Hot Encoding unique categories: {len(encoder.categories_[0])}\")  # type: ignore\n",
    "logger.info(f\"Standard Scaling mean: {scaler.mean_[0]:.4f}\")  # type: ignore\n",
    "logger.info(f\"Standard Scaling std: {scaler.scale_[0]:.4f}\")  # type: ignore\n",
    "logger.info(f\"Feature Hashing features count: {train_source.shape[1]}\")\n",
    "logger.info(f\"Total number of features: {train_features.shape[1]}\\n\")\n",
    "\n",
    "# write features and models to files\n",
    "save_npz(FEATURES_TRAIN, train_features)\n",
    "save_npz(FEATURES_TEST, test_features)\n",
    "_ = dump(vectorizer, VECTORIZER_MODEL)\n",
    "_ = dump(encoder, ENCODER_MODEL)\n",
    "_ = dump(scaler, SCALER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time\n",
      "         1           0.7654            2.90m\n",
      "         2           0.6565            2.77m\n",
      "         3           0.5758            2.86m\n",
      "         4           0.5121            3.03m\n",
      "         5           0.4581            2.95m\n",
      "         6           0.4140            2.85m\n",
      "         7           0.3766            2.78m\n",
      "         8           0.3440            2.73m\n",
      "         9           0.3160            2.67m\n",
      "        10           0.2917            2.65m\n",
      "        20           0.1581            2.34m\n",
      "        30           0.0744            2.01m\n",
      "        40           0.0528            1.74m\n",
      "        50           0.0317            1.45m\n",
      "        60           0.0204            1.16m\n",
      "        70           0.0173           52.18s\n",
      "        80           0.0140           34.81s\n",
      "        90           0.0126           17.30s\n",
      "       100           0.0086            0.00s\n",
      "Model trained complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load features (to prevent forced extraction to define variables)\n",
    "train_features = load_npz(FEATURES_TRAIN)\n",
    "train_labels = np.load(LABELS_TRAIN)\n",
    "\n",
    "# train model\n",
    "model = GradientBoostingClassifier(verbose=1)\n",
    "with log_redirector(logger):\n",
    "    model.fit(train_features, train_labels)  # type: ignore\n",
    "logger.info(\"Model trained complete\\n\")\n",
    "\n",
    "# write model to file\n",
    "_ = dump(model, GBM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8341019057652395\n",
      "Confusion matrix:\n",
      "[[ 63467   1863]\n",
      " [ 50733 200975]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.97      0.71     65330\n",
      "           1       0.99      0.80      0.88    251708\n",
      "\n",
      "    accuracy                           0.83    317038\n",
      "   macro avg       0.77      0.88      0.80    317038\n",
      "weighted avg       0.90      0.83      0.85    317038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and features (to prevent forced training to define variables)\n",
    "model = load(GBM_MODEL)\n",
    "test_features = load_npz(FEATURES_TEST)\n",
    "test_labels = np.load(LABELS_TEST)\n",
    "\n",
    "# evaluate model\n",
    "predictions = model.predict(test_features)  # type: ignore\n",
    "accuracy = metrics.accuracy_score(test_labels, predictions)\n",
    "logger.info(f\"Train accuracy: {accuracy}\")\n",
    "conf_matrix = metrics.confusion_matrix(test_labels, predictions)\n",
    "logger.info(f\"Confusion matrix:\\n{conf_matrix}\")\n",
    "report = metrics.classification_report(test_labels, predictions)\n",
    "logger.info(f\"Classification report:\\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
