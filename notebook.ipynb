{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bluetooth DoS Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ensure that the working directory is the same as the notebook\n",
    "notebook_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "os.chdir(notebook_path)\n",
    "\n",
    "# create directories\n",
    "if not os.path.exists(\"data\"):\n",
    "    raise Exception(\"Data directory not found.\")\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "ATTACK_TRAIN = 'data/dos_train.csv'\n",
    "BENIGN_TRAIN = 'data/benign_train.csv'\n",
    "ATTACK_TEST = 'data/dos_test.csv'\n",
    "BENIGN_TEST = 'data/benign_test.csv'\n",
    "\n",
    "# preprocessed data paths\n",
    "PREPROCESSED_TRAIN = 'data/preprocessed_train.csv'\n",
    "PREPROCESSED_TEST = 'data/preprocessed_test.csv'\n",
    "LABELS_TRAIN = 'data/labels_train.csv'\n",
    "LABELS_TEST = 'data/labels_test.csv'\n",
    "\n",
    "# features paths\n",
    "FEATURES_TEST = 'data/features_test.csv'\n",
    "FEATURES_TRAIN = 'data/features_train.csv'\n",
    "\n",
    "# models paths\n",
    "VECTORIZER_MODEL = 'models/vectorizer.joblib'\n",
    "ENCODER_MODEL = 'models/encoder.joblib'\n",
    "SCALER_MODEL = 'models/scaler.joblib'\n",
    "GBM_MODEL = 'models/gbm.joblib'  # gradient boosting machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "The dataset is obtained from the following link:\n",
    "https://www.unb.ca/cic/datasets/iomt-dataset-2024.html\n",
    "\n",
    "The specific dataset used is the \"Bluetooth\" dataset. The dataset is in `.pcap` format. The dataset is first converted to `.csv` format using the `tshark` command line tool (WireShark can also be used). The resulting dataset files are:\n",
    "\n",
    "- `benign_test.csv`\n",
    "- `benign_train.csv`\n",
    "- `dos_test.csv`\n",
    "- `dos_train.csv`\n",
    "\n",
    "The original dataset are compressed in their `.pcap` format at `data/bl_dataset.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing datasets...\n",
      "Training data:\n",
      "                 No.          Time         Length      Type\n",
      "count  998391.000000  9.983910e+05  998391.000000  998391.0\n",
      "mean   499196.000000  4.621494e+05      20.064474       1.0\n",
      "std    288210.800641  4.679402e+05      12.352341       0.0\n",
      "min         1.000000  0.000000e+00       4.000000       1.0\n",
      "25%    249598.500000  2.276077e+03       8.000000       1.0\n",
      "50%    499196.000000  6.853190e+05      16.000000       1.0\n",
      "75%    748793.500000  8.540422e+05      32.000000       1.0\n",
      "max    998391.000000  1.198804e+06     255.000000       1.0\n",
      "\n",
      "Testing data:\n",
      "                 No.           Time         Length      Type\n",
      "count  251708.000000  251708.000000  251708.000000  251708.0\n",
      "mean   125854.500000    1071.256077      20.006591       1.0\n",
      "std     72661.985116     638.578923      12.006976       0.0\n",
      "min         1.000000       0.000000       5.000000       1.0\n",
      "25%     62927.750000     601.485238       8.000000       1.0\n",
      "50%    125854.500000    1050.763037      16.000000       1.0\n",
      "75%    188781.250000    1669.751179      32.000000       1.0\n",
      "max    251708.000000    2094.097550      46.000000       1.0\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "attack_train = pd.read_csv(ATTACK_TRAIN)\n",
    "benign_train = pd.read_csv(BENIGN_TRAIN)\n",
    "attack_test = pd.read_csv(ATTACK_TEST)\n",
    "benign_test = pd.read_csv(BENIGN_TEST)\n",
    "\n",
    "# add type column indicating attack or benign\n",
    "attack_train[\"Type\"] = 1\n",
    "attack_test[\"Type\"] = 1\n",
    "benign_train[\"Type\"] = 0\n",
    "benign_test[\"Type\"] = 0\n",
    "\n",
    "# combine datasets\n",
    "train_dataset = pd.concat([attack_train, benign_train], ignore_index=True)\n",
    "test_dataset = pd.concat([attack_test, benign_test], ignore_index=True)\n",
    "\n",
    "# shuffle datasets\n",
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# generate labels\n",
    "train_labels = train_dataset[\"Type\"]\n",
    "train_dataset.drop(columns=[\"Type\"], inplace=True)\n",
    "test_labels = test_dataset[\"Type\"]\n",
    "test_dataset.drop(columns=[\"Type\"], inplace=True)\n",
    "\n",
    "# summary statistics\n",
    "print(\"Summarizing datasets...\")\n",
    "print(f\"Training data:\\n{attack_train.describe()}\\n\")\n",
    "print(f\"Testing data:\\n{attack_test.describe()}\")\n",
    "\n",
    "# write modified dataset to files\n",
    "train_dataset.to_csv(PREPROCESSED_TRAIN, index=False)\n",
    "test_dataset.to_csv(PREPROCESSED_TEST, index=False)\n",
    "np.save(LABELS_TRAIN, train_labels)\n",
    "np.save(LABELS_TEST, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt-venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
